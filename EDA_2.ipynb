{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from boruta import BorutaPy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./datasets/input/fraudTrain.csv')\n",
    "df_test = pd.read_csv('./datasets/input/fraudTest.csv')\n",
    "prueba = pd.read_csv('./streamlit_app/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import zipfile\n",
    "from catboost import CatBoostClassifier\n",
    "import streamlit as st\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "def fraud_pct_by_column(data, column_name, target_name, fraud_pct_col_name, rank_col_name):\n",
    "\n",
    "    group_fraud_by_column = data.groupby(column_name).agg(\n",
    "      total_sales=(target_name, 'count'),\n",
    "      fraud_sales=(target_name, 'sum')\n",
    "    )\n",
    "\n",
    "    # Calcular el porcentaje de fraude para cada valor\n",
    "    group_fraud_by_column[fraud_pct_col_name] = (group_fraud_by_column['fraud_sales'] / group_fraud_by_column['total_sales']) * 100\n",
    "    group_fraud_by_column = group_fraud_by_column.reset_index()\n",
    "\n",
    "    # Rank de porcentaje de fraude\n",
    "    group_fraud_by_column[rank_col_name] = group_fraud_by_column[fraud_pct_col_name].rank(ascending=False)\n",
    "\n",
    "    # Unirlo con el df original\n",
    "    data = data.merge(group_fraud_by_column[[column_name, fraud_pct_col_name, rank_col_name]], on=column_name,how='left')\n",
    "\n",
    "    return data, group_fraud_by_column[[column_name, fraud_pct_col_name, rank_col_name]]\n",
    "\n",
    "# Se agruparan las profesiones para disminuir la dimensionalidad\n",
    "def assign_sector(x):\n",
    "    group_jobs = {\n",
    "        \"Engineering and Technology\": [\"engineer\", \"developer\", \"programmer\", \"technician\", \"architect\", \"systems\", \n",
    "                                    \"network\", \"administrator\", \"data scientist\", \"cybersecurity\", \"web developer\", \n",
    "                                    \"analyst\", \"database\", \"devops\", \"maintenance\", \"manufacturing\", \"site\", \n",
    "                                    \"structural\", \"materials\", \"biomedical\", \"environmental\", \"telecommunications\"],\n",
    "        \n",
    "        \"Healthcare and Medicine\": [\"doctor\", \"nurse\", \"therapist\", \"pharmacist\", \"health\", \"surgeon\", \"dentist\", \n",
    "                                    \"clinician\", \"physician\", \"optometrist\", \"radiologist\", \"paramedic\", \"midwife\", \n",
    "                                    \"veterinarian\", \"psychiatrist\", \"psychologist\", \"radiographer\", \"biochemist\", \n",
    "                                    \"cytogeneticist\", \"audiologist\", \"pathologist\"],\n",
    "        \n",
    "        \"Education and Training\": [\"teacher\", \"professor\", \"educator\", \"trainer\", \"lecturer\", \"scientist\", \"tutor\", \n",
    "                                \"principal\", \"instructor\", \"counselor\", \"academic\", \"researcher\", \"dean\", \n",
    "                                \"headmaster\", \"careers adviser\", \"museum education officer\", \"education administrator\"],\n",
    "        \n",
    "        \"Science and Environment\": [\"scientist\", \"environmental consultant\", \"ecologist\", \"geologist\", \"hydrologist\", \n",
    "                                    \"conservation officer\", \"horticulturist\", \"geophysicist\", \"soil scientist\", \n",
    "                                    \"agricultural consultant\", \"agricultural engineer\", \"oceanographer\", \n",
    "                                    \"fisheries officer\"],\n",
    "        \n",
    "        \"Art, Design, and Media\": [\"designer\", \"artist\", \"animator\", \"photographer\", \"film editor\", \"video editor\", \n",
    "                                \"television producer\", \"film producer\", \"radio producer\", \"curator\"],\n",
    "        \n",
    "        \"Finance\": [\"analyst\", \"accountant\", \"auditor\", \"banker\", \"financial\", \"investment\", \"controller\", \"broker\", \n",
    "                    \"consultant\", \"treasurer\", \"loan officer\", \"trader\", \"actuary\", \"economist\", \"portfolio\", \"credit\"],\n",
    "        \n",
    "        \"Marketing\": [\"manager\", \"executive\", \"specialist\", \"consultant\", \"advertising\", \"public relations\", \"strategist\", \n",
    "                    \"director\", \"coordinator\", \"brand\", \"SEO\", \"content\", \"digital\", \"market research\", \"social media\", \n",
    "                    \"copywriter\"],\n",
    "        \n",
    "        \"Manufacturing\": [\"operator\", \"mechanic\", \"assembler\", \"fabricator\", \"engineer\", \"technician\", \"welder\", \n",
    "                        \"planner\", \"quality\", \"machinist\", \"production\", \"inspector\", \"supervisor\", \"foreman\", \n",
    "                        \"toolmaker\", \"CNC\"],\n",
    "        \n",
    "        \"Retail\": [\"cashier\", \"salesperson\", \"store\", \"associate\", \"manager\", \"clerk\", \"shopkeeper\", \"merchandiser\", \n",
    "                \"assistant\", \"retail\", \"customer service\", \"sales\", \"inventory\", \"buyer\", \"stocker\", \"checkout\"],\n",
    "        \n",
    "        \"Legal\": [\"lawyer\", \"attorney\", \"paralegal\", \"judge\", \"legal\", \"solicitor\", \"notary\", \"clerk\", \"litigator\", \n",
    "                \"advocate\", \"barrister\", \"counsel\", \"magistrate\", \"prosecutor\", \"defense\", \"compliance\"],\n",
    "        \n",
    "        \"Hospitality\": [\"chef\", \"waiter\", \"bartender\", \"host\", \"manager\", \"receptionist\", \"housekeeper\", \"concierge\", \n",
    "                        \"caterer\", \"cook\", \"hotel\", \"tour guide\", \"event planner\", \"sous chef\", \"sommelier\", \"valet\"],\n",
    "        \n",
    "        \"Construction\": [\"builder\", \"carpenter\", \"electrician\", \"plumber\", \"architect\", \"project manager\", \"site manager\", \n",
    "                        \"surveyor\", \"foreman\", \"bricklayer\", \"roofer\", \"civil engineer\", \"construction\", \"contractor\", \n",
    "                        \"inspector\", \"draftsman\"]\n",
    "    }\n",
    "    for key in group_jobs:\n",
    "        for role in group_jobs[key]:\n",
    "            if x.find(role) != -1:\n",
    "                return key\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def datetime_split(data, datatime_col_name, day_col_name, month_col_name, year_col_name, hour_col_name, weekday_col_name):\n",
    "    # Transformar la columna a datetime para el procesado\n",
    "    data[datatime_col_name] = pd.to_datetime(data[datatime_col_name])\n",
    "\n",
    "    # Dividir el día del mes, mes, año, hora y día de la semana en nuevas columnas\n",
    "    data[day_col_name] = data[datatime_col_name].dt.day\n",
    "    data[month_col_name] = data[datatime_col_name].dt.month\n",
    "    data[year_col_name] = data[datatime_col_name].dt.year\n",
    "    data[hour_col_name] = data[datatime_col_name].dt.hour\n",
    "    data[weekday_col_name] = data[datatime_col_name].dt.weekday\n",
    "\n",
    "    return data\n",
    "\n",
    "def dob_to_age(data, dob_col_name, age_col_name):\n",
    "    # Transformar la columna dob de object a datetime\n",
    "    data[dob_col_name] = pd.to_datetime(data[dob_col_name])\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    actual_date = pd.to_datetime(datetime.now().date())\n",
    "\n",
    "    # Convertir la fecha de nacimiento a edad [años] (fecha actual - fecha de nacimiento)/días promedio por año\n",
    "    data[age_col_name] = ((actual_date - data[dob_col_name]).dt.days / 365.25).astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Función para calcular la distancia\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radio de la tierra [km]\n",
    "    lat1_rad, lon1_rad = radians(lat1), radians(lon1)\n",
    "    lat2_rad, lon2_rad = radians(lat2), radians(lon2)\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def extract_zip_to_csv(uploaded_file, temp_dir):\n",
    "    \"\"\"\n",
    "    Extrae un archivo zip, busca un archivo CSV dentro y lo convierte a csv.\n",
    "    \"\"\"\n",
    "    zip_path = os.path.join(temp_dir, \"temp.zip\")\n",
    "\n",
    "    # Guardar el archivo zip subido en el directorio temporal\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.getvalue())\n",
    "\n",
    "    # Descomprimir el archivo zip\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "\n",
    "    # Buscar archivos CSV en el directorio temporal\n",
    "    extracted_files = [f for f in os.listdir(temp_dir) if f.endswith('.csv')]\n",
    "\n",
    "    return extracted_files\n",
    "\n",
    "\n",
    "def catboost_model(features_scaled, target, model):\n",
    "    predictions = model.predict(features_scaled)                   # Hacer predicciones\n",
    "\n",
    "    # Mostrar las predicciones\n",
    "    st.write(\"Mostrando las primeras 5 predicciones:\")\n",
    "    st.dataframe(pd.DataFrame(predictions, columns=['Predicción']).head())\n",
    "\n",
    "    # Mostrar el objetivo real\n",
    "    st.write(\"Mostrando las primeras 5 reales:\")\n",
    "    st.dataframe(target.head())\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = accuracy_score(target, predictions)\n",
    "    report = classification_report(target, predictions)\n",
    "    return predictions, accuracy, report\n",
    "\n",
    "def extract_zip_to_model(zip_file, name_model):\n",
    "    try:\n",
    "        # Extraer el archivo .cbm dentro de un directorio temporal\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                # Filtrar y extraer solo el archivo de modelo necesario\n",
    "                extracted_files = []\n",
    "                for file_name in zip_ref.namelist():\n",
    "                    if file_name.endswith(name_model):\n",
    "                        zip_ref.extract(file_name, tmpdirname)\n",
    "                        extracted_file_path = os.path.join(tmpdirname, file_name)\n",
    "                        extracted_files.append(extracted_file_path)\n",
    "                        break\n",
    "                else:\n",
    "                    raise FileNotFoundError(f'{name_model} no encontrado en el archivo ZIP.')\n",
    "\n",
    "                # Imprimir la ruta del archivo extraído y los archivos en el directorio temporal\n",
    "                print(f\"Archivo extraído en: {extracted_files[0]}\")\n",
    "                print(f\"Archivos en el directorio temporal: {os.listdir(tmpdirname)}\")\n",
    "\n",
    "                # Cargar el modelo CatBoost\n",
    "                model = CatBoostClassifier()\n",
    "                model.load_model(extracted_files[0])\n",
    "                return model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer o cargar el modelo: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3040964631.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def preprocessing_data(data, create_data=False)\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_data(data, create_data=False):\n",
    "   # Añadir columnas de porcentaje de fraude y ranking para: vendedor, ciudad y estado\n",
    "   data, group_fraud_by_merch = fraud_pct_by_column(data, 'merchant', 'is_fraud', 'fraud_merch_pct', 'fraud_merch_rank')\n",
    "   data, group_fraud_by_city = fraud_pct_by_column(data, 'city', 'is_fraud', 'fraud_city_pct', 'fraud_city_rank')\n",
    "   data, group_fraud_by_state = fraud_pct_by_column(data, 'state', 'is_fraud', 'fraud_state_pct', 'fraud_state_rank')\n",
    "\n",
    "\n",
    "\n",
    "   # Aplicar la función de reemplazo de profesiones por sector (Para visualización) y realizar un encoded por la frecuencia (para el modelo)\n",
    "   data['job_sector'] = data['job'].apply(assign_sector)\n",
    "   job_freq = data['job'].value_counts(normalize=True)\n",
    "\n",
    "   # Mapear el encoded\n",
    "   data['job_encoded'] = data['job'].map(job_freq)\n",
    "\n",
    "   # Dividir la columna de la fecha/hora en columnas separadas para: día del mes, mes, año, hora, día de la semana. \n",
    "   data = datetime_split(data, 'trans_date_trans_time', 'trans_day', 'trans_month', 'trans_year', 'trans_hour', 'trans_weekday')\n",
    "\n",
    "   # Transformar la fecha de nacimiento en edad\n",
    "   data = dob_to_age(data, 'dob', 'age')\n",
    "\n",
    "   # Crear una nueva columna con la distancia entre el vendedor y el comprador\n",
    "   data[\"distance_to_merch\"] = data.apply(lambda row: haversine_distance(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
    "\n",
    "   # Inicializar el codificador OneHotEncoder\n",
    "   encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "\n",
    "   # Ajustar el codificador con los datos de entrenamiento\n",
    "   data_ohe = encoder.fit_transform(data[['category', 'gender']])\n",
    "\n",
    "   # Reconstruir el dataframe\n",
    "   col_names = ['category_food_dining', 'category_gas_transport',\n",
    "      'category_grocery_net', 'category_grocery_pos',\n",
    "      'category_health_fitness', 'category_home', 'category_kids_pets',\n",
    "      'category_misc_net', 'category_misc_pos', 'category_personal_care',\n",
    "      'category_shopping_net', 'category_shopping_pos', 'category_travel',\n",
    "      'gender_M']\n",
    "   data_ohe = pd.DataFrame(data_ohe, columns=col_names)\n",
    "   data_ohe = pd.concat([data, data_ohe], axis=1)\n",
    "\n",
    "   # Eliminar columnas redundantes o con poca información para el modelo\n",
    "   data_ohe.drop(columns=['Unnamed: 0', 'cc_num', 'first', 'last', 'street', 'unix_time', 'trans_num',\n",
    "                           'merchant', 'city', 'state', 'job', 'job_sector', 'trans_date_trans_time', \n",
    "                           'dob', 'lat', 'long', 'merch_lat', 'merch_long', 'category', 'gender'], axis=1, inplace=True)\n",
    "\n",
    "   # Guardar los datos para el procesado de nuevos dataframes\n",
    "   if create_data:\n",
    "      group_fraud_by_merch.to_csv('streamlit_app/group_fraud_by_merch.csv')\n",
    "      group_fraud_by_city.to_csv('streamlit_app/group_fraud_by_city.csv')\n",
    "      group_fraud_by_state.to_csv('streamlit_app/group_fraud_by_state.csv')\n",
    "      job_freq.to_csv('streamlit_app/job_freq.csv', index=True)\n",
    "      joblib.dump(encoder, './streamlit_app/onehotencoder.pkl')\n",
    "\n",
    "   return data_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "prueba = preprocessing_data(prueba)\n",
    "prueba.drop(columns=['Unnamed: 0.1', 'is_fraud'], axis=1, inplace=True)\n",
    "prueba.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = preprocessing_data(df_combined, create_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir los datos en características y objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(data_clean.drop('is_fraud', axis=1), \n",
    "                                                                            data_clean['is_fraud'], \n",
    "                                                                            test_size = 0.2, \n",
    "                                                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escalar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas a escalar\n",
    "cols_to_scale=['amt', 'zip', 'city_pop', 'fraud_merch_pct', 'fraud_merch_rank', \n",
    "                'fraud_city_pct', 'fraud_city_rank', 'fraud_state_pct', 'fraud_state_rank',\n",
    "                'job_encoded', 'trans_day', 'trans_month', 'trans_year', 'trans_hour', \n",
    "                'trans_weekday', 'age', 'distance_to_merch']\n",
    "\n",
    "# Inicializar el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Escalar las características de entrenamiento\n",
    "features_train_scaled = features_train.copy()\n",
    "features_train_scaled[cols_to_scale] = scaler.fit_transform(features_train_scaled[cols_to_scale])\n",
    "\n",
    "# Escalar las características de prueba\n",
    "features_test_scaled = features_test.copy()\n",
    "features_test_scaled[cols_to_scale] = scaler.transform(features_test_scaled[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "prueba_scaled = prueba.copy()\n",
    "prueba_scaled[cols_to_scale] = scaler.transform(prueba_scaled[cols_to_scale])\n",
    "prueba_scaled.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_scaled.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_scaled.to_csv('streamlit_app/features_test_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el escalador para usarlo en el conjunto de prueba\n",
    "joblib.dump(scaler, 'streamlit_app/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancear los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE en el conjunto de entrenamiento\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "features_train_resampled, target_train_resampled = smote.fit_resample(features_train_scaled, target_train)\n",
    "\n",
    "# Revisar la nueva distribución de clases en el conjunto de entrenamiento balanceado\n",
    "print(\"Distribución en y_train_resampled:\\n\", target_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenaer el modelo\n",
    "# model_logistic_reg = LogisticRegression(max_iter=200)\n",
    "# model_logistic_reg.fit(features_train_resampled.values, target_train_resampled)\n",
    "# predict_logistic_reg = model_logistic_reg.predict(features_test_scaled.values)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# accuracy = accuracy_score(target_test, predict_logistic_reg)\n",
    "# report = classification_report(target_test, predict_logistic_reg)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print('Classification Report:')\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.8969955112165602\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.90      0.95    368526\n",
    "           1       0.04      0.83      0.08      1953\n",
    "\n",
    "    accuracy                           0.90    370479\n",
    "   macro avg       0.52      0.86      0.51    370479\n",
    "weighted avg       0.99      0.90      0.94    370479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo\n",
    "model_catboost = CatBoostClassifier(task_type='GPU', verbose=0, random_state=42)\n",
    "\n",
    "# Definir el rango de parámetros a buscar\n",
    "param_grid = {\n",
    "    'bagging_temperature': [0],\n",
    "    'boosting_type': ['Plain'],  # Puedes intentar 'Plain' también si es necesario\n",
    "    'border_count': [60],  # Puedes ajustar según tus datos\n",
    "    'depth': [8],  # Aumentar si se necesita más capacidad del modelo   16 ES EL MEJOR\n",
    "    'grow_policy': ['Depthwise'],  # Puedes probar otros si es necesario\n",
    "    'iterations': [500],  # Ajustar según tus necesidades y tiempo\n",
    "    'l2_leaf_reg': [7],  # Regularización\n",
    "    'learning_rate': [0.2],  # Ajustar según el rendimiento del modelo\n",
    "    'random_strength': [5]  # Ajustar si se necesita más aleatoriedad\n",
    "}\n",
    "\n",
    "\n",
    "# Definir la validación cruzada estratificada\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search_catboost = GridSearchCV(estimator=model_catboost, param_grid=param_grid, \n",
    "                           cv=stratified_kfold, verbose=2, error_score='raise')\n",
    "\n",
    "# Ajustar el modelo\n",
    "grid_search_catboost.fit(features_train_resampled.values, target_train_resampled)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_catboost = grid_search_catboost.best_estimator_\n",
    "\n",
    "# Hacer predicciones\n",
    "predicts_catboost = best_model_catboost.predict(features_test_scaled.values)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = accuracy_score(target_test, predicts_catboost)\n",
    "report = classification_report(target_test, predicts_catboost)\n",
    "\n",
    "print(f'Best Parameters: {grid_search_catboost.best_params_}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'bagging_temperature': 0, 'boosting_type': 'Plain', 'border_count': 60, 'depth': 4, 'grow_policy': 'Depthwise', 'iterations': 500, 'l2_leaf_reg': 7, 'learning_rate': 0.2, 'random_strength': 5}\n",
    "Accuracy: 0.9987745594217217\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    368526\n",
    "           1       0.90      0.87      0.88      1953\n",
    "\n",
    "    accuracy                           1.00    370479\n",
    "   macro avg       0.95      0.93      0.94    370479\n",
    "weighted avg       1.00      1.00      1.00    370479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'bagging_temperature': 0, 'boosting_type': 'Plain', 'border_count': 60, 'depth': 4, 'grow_policy': 'Depthwise', 'iterations': 100, 'l2_leaf_reg': 7, 'learning_rate': 0.2, 'random_strength': 5}\n",
    "Accuracy: 0.9973790687191447\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    368526\n",
    "           1       0.70      0.87      0.78      1953\n",
    "\n",
    "    accuracy                           1.00    370479\n",
    "   macro avg       0.85      0.94      0.89    370479\n",
    "weighted avg       1.00      1.00      1.00    370479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'bagging_temperature': 0, 'boosting_type': 'Plain', 'border_count': 60, 'depth': 6, 'grow_policy': 'Depthwise', 'iterations': 500, 'l2_leaf_reg': 7, 'learning_rate': 0.2, 'random_strength': 5}\n",
    "Accuracy: 0.9990957652120633\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    368526\n",
    "           1       0.95      0.88      0.91      1953\n",
    "\n",
    "    accuracy                           1.00    370479\n",
    "   macro avg       0.97      0.94      0.96    370479\n",
    "weighted avg       1.00      1.00      1.00    370479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "best_model_catboost.save_model('streamlit_app/catboost_bestmodel.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
